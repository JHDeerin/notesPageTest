<html>
<head>
<meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0" name="viewport"/>
<title>Jake's CS Notes - Knowledge-Based AI</title>
<link href="https://fonts.googleapis.com/css?family=Inconsolata" rel="stylesheet"/>
<link href="../../css/testStyle.css" rel="stylesheet"/>
<link href="../../css/notePageStyle.css" rel="stylesheet"/>
<link href="../../css/cs4635Theme.css" id="class-theme-styles" rel="stylesheet"/>
<link crossorigin="anonymous" href="https://use.fontawesome.com/releases/v5.0.13/css/all.css" integrity="sha384-DNOHZ68U8hZfKXOrtjWvjxusGo9WQnrNx2sqG0tfsghAvtVlRW3tvkXWZh58N9jp" rel="stylesheet"/>
</head>
<body>
<script defer="" src="../../js/wrapText.js"></script>
<script defer="" src="../../js/pageTransitions.js"></script>
<script async="" defer="" src="../../js/loadMathJax.js"></script>
<nav class="nav-top">
<ul>
<li class="link-with-slash"><a href="../../index.html"><i class="fas fa-home"></i></a></li>
<li><a href="#" id="class-title-link">Knowledge-Based AI</a></li>
</ul>
<ul class="note-links-slider"><li><a class="is-note-link" href="0_firstDay_8_19_19.html">0. First Day</a></li><li><a class="is-note-link" href="1_introKBAI_8_21_19.html">1. Introduction to KBAI (cont.)</a></li><li><a class="is-note-link" href="2_principlesOfKBAI_8_23_19.html">2. Principles of KBAI</a></li><li><a class="is-note-link" href="3_introFrames_8_26_19.html">3. Introduction to Frames</a></li><li><a class="is-note-link" href="4_moreFrames_8_28_19.html">4. Frames (cont.)</a></li><li><a class="is-note-link" href="5_semanticNets_8_30_19.html">5. Semantic Networks</a></li><li><a class="is-note-link" href="6_optionsGenTest_9_4_19.html">6. Choosing Options / Generate and Test</a></li><li><a class="is-note-link" href="7_genTestMeansEnd_9_6_19.html">7. Generate and Test / Means-End Analysis</a></li><li><a class="is-note-link" href="8_productionSys_9_9_19.html">8. Production Systems</a></li><li><a class="is-note-link" href="9_moreProductionSys_9_11_19.html">9. Production Systems</a></li><li><a class="is-note-link" href="10_caseBasedReasoning_9_13_19.html">10. Case-Based Reasoning</a></li><li><a class="is-note-link" href="11_moreCaseBased_9_16_19.html">11. Case-Based Reasoning (cont.)</a></li><li><a class="is-note-link" href="13_introToClassification_9_20_19.html">12. Classification Basics</a></li><li><a class="is-note-link" href="14_incrementalConceptLearning_9_23_19.html">13. Incremental Concept Learning</a></li><li><a class="is-note-link" href="15_conceptLogic_9_25_19.html">14. Concept Learning (cont.) / Logic</a></li><li><a class="is-note-link" href="16_moreLogic_9_30_19.html">15. Logic (cont.)</a></li><li><a class="is-note-link" href="17_evenMoreLogicPlanning_10_2_19.html">16. Logic (cont.) / Planning</a></li><li><a class="is-note-link" href="18_morePlanning_10_4_19.html">17. Planning (cont.)</a></li><li><a class="is-note-link" href="19_knowledgeEngineering_10_7_19.html">18. Knowledge Engineering</a></li><li><a class="is-note-link" href="20_yetMorePlanning_10_9_19.html">19. Planning (cont.)</a></li><li><a class="is-note-link" href="21_understandingCommonSense_10_11_19.html">20. Understanding and Common-Sense</a></li><li><a class="is-note-link" href="22_commonSense_10_16_19.html">21. Common-Sense Reasoning (cont.)</a></li><li><a class="is-note-link" href="23_fractalReasoning_10_18_19.html">22. Fractal Reasoning</a></li><li><a class="is-note-link" href="24_explanationReasoning_10_21_19.html">23. Explanation-Based Reasoning</a></li><li><a class="is-note-link" href="25_analogicalReasoning_10_23_19.html">24. Analogical Reasoning</a></li><li class="active-note-page"><a class="is-note-link" href="26_moreAnalogicalReasoning_10_25_19.html">25. Analogical Reasoning (cont.)</a></li><li><a class="is-note-link" href="27_moreAnalogyVersionSpace_10_28_19.html">26. Analogical Reason (cont). / Version Spaces</a></li><li><a class="is-note-link" href="28_versionSpaces_10_30_19.html">27. Version Spaces</a></li><li><a class="is-note-link" href="29_constraintProp_11_1_19.html">28. Constraint Propagation</a></li><li><a class="is-note-link" href="30_moreConstraintProp_11_4_19.html">29. Constraint Propagation (cont.)</a></li><li><a class="is-note-link" href="31_diagnosis_11_6_19.html">30. Diagnosis</a></li><li><a class="is-note-link" href="32_mistakeCorrection_11_8_19.html">31. Learning from Mistakes</a></li><li><a class="is-note-link" href="33_metacognition_11_11_19.html">32. Metacognition</a></li><li><a class="is-note-link" href="34_advancedTopics_11_13_19.html">33. Advanced Topics</a></li><li><a class="is-note-link" href="35_principles_11_15_19.html">34. Principles of AI</a></li><li><a class="is-note-link" href="36_wrapUp_11_18_19.html">35. Wrapping Up</a></li></ul>
</nav>
<main>
<a class="side-link is-note-link" href="25_analogicalReasoning_10_23_19.html"></a>
<article>
<!-- Actual note text goes into 'pre' -->
<pre id="text-width-ruler"></pre>
<pre class="main-note-text">//****************************************************************************//
//*********** Analogical Reasoning (cont.) - October 25th, 2019 *************//
//**************************************************************************//

- I am always game for incorporating Monty Python into class

- We've been talking about analogies in class, but can analogies be wrong? YES, they can!
    - *cue "Burn the Witch" clip from Monty Python and the Holy Grail*
    - Here, the Monty Python crew makes an invalid analogy: because witches catch on fire, and wood also catches on fire, they decide the important thing is that witches are made of wood!
        - As our analogies compare ever more different things, we need more sophisticated ways of identifying relationships - ways that, frankly, AI researchers haven't come up with yet
    - How can we make analogies?
        - Almost all AI research today has been done at the level of recording cases (which is what most machine learning techniques boil down to); some work has been done on intermediate techniques, but relational reasoning? That's proven immensely difficult, and precious little progress has been made
--------------------------------------------------------------------------------

- So, what does an analogy even look like?
    - First, RETRIEVAL is when we bring to mind a memory of something we know about that's somewhat similar to the new thing we're dealing with
    - Then MAPPING is when we draw a potential connection between that thing we know and something knew
    - We then TRANSFER that mapping, knowledge of what is known to something new ("the planets in the solar system revolve around the sun, and the nucleus in an atom is like the sun")
    - We then EVALUATE the analogy to try and see if it's valid
    - Finally, if that analogy is good, we STORE it
        - We use analogies for almost everything; think of how often you use metaphors without noticing it (e.g. "We broke up" - but you didn't literally fall in half!)
        - At the same time, analogies don't necessarily give us right answers; people will use different, incompatible analogies for the same thing

- Retrieval we've talked about, as well as drawing superficial similarities based on features - but how can we deal with mapping and transfer in deep relationships?
    - Here, we need to identify the relationships in a problem (which is already hard) and then somehow map those relationships to a new problem
    - What kinds of similarities are there? We'll talk about 3:
        - SEMANTIC SIMILARITY is when we draw some conceptual connection between a past and present scenario
        - PRAGMATIC SIMILARITY is when two things involve similar external factors, like goals (e.g. a general trying to defeat an army is like a patient trying to defeat cancer)
        - STRUCTURAL SIMILARITY means that the representational structures we use between two things are similar (e.g. a similar structure between atoms and the solar system)
            - In the last 40 years of AI, a position called SYSTEMATICY has arisen: that humans tend to focus about concepts that are most nested (not sure I understand what nested refers to?)
                - I *think* nested refers to the structure of the concept, where (for instance) the idea of attractive forces applies to the nucleus attracting electrons, which is deeper than the mere statement that "electrons go around the nucleus"
            - This might also explain why different people make different analogies: since everyone represents concepts slightly differently, they'll come to different conclusions about how similar certain ideas are!
    - Let's look more deeply at the atom-solar system analogy - what are the "deeper" similarities?
        - (on the slides)

- So, unlike low-level intelligence, analogies are a high-level heuristic method that isn't guaranteed to give us correct conclusions

- What we want to do with analogy making, now, is this: given a target concept or case, we want to select an appropriate "source" case, map what relevant elements are similar between the two, transfer the key relationships from the source to the target (and possibly vice-versa), and then store our conclusion if it's fitting
    - There are some experimental programs from the last 25 years that've made inroads here

- How we do this may seem like magic right now - we'll go into it more next week, but please watch the videos! They'll help you understand!
    - Alright, that's all for today - don't forget to work on your 2nd projects, and have a good weekend!</pre>
</article>
<a class="side-link is-note-link" href="27_moreAnalogyVersionSpace_10_28_19.html"></a>
</main>
</body>
</html>