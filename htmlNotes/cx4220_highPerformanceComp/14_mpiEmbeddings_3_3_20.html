<html>
<head>
<meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0" name="viewport"/>
<title>Jake's CS Notes - Intro. to HPC</title>
<link href="https://fonts.googleapis.com/css?family=Inconsolata" rel="stylesheet"/>
<link href="../../css/testStyle.css" rel="stylesheet"/>
<link href="../../css/notePageStyle.css" rel="stylesheet"/>
<link href="../../css/cx4220Theme.css" id="class-theme-styles" rel="stylesheet"/>
<link crossorigin="anonymous" href="https://use.fontawesome.com/releases/v5.0.13/css/all.css" integrity="sha384-DNOHZ68U8hZfKXOrtjWvjxusGo9WQnrNx2sqG0tfsghAvtVlRW3tvkXWZh58N9jp" rel="stylesheet"/>
</head>
<body>
<script defer="" src="../../js/wrapText.js"></script>
<script defer="" src="../../js/pageTransitions.js"></script>
<script async="" defer="" src="../../js/loadMathJax.js"></script>
<nav class="nav-top">
<ul>
<li class="link-with-slash"><a href="../../index.html"><i class="fas fa-home"></i></a></li>
<li><a href="#" id="class-title-link">Intro. to HPC</a></li>
</ul>
<ul class="note-links-slider"><li><a class="is-note-link" href="0_firstDay_1_7_20.html">0. First Day</a></li><li><a class="is-note-link" href="1_parallelGoals_1_9_20.html">1. Parallel Algorithm Design Goals</a></li><li><a class="is-note-link" href="2_algoExamplesMPI_1_14_20.html">2. Algorithm Examples / Intro. to MPI</a></li><li><a class="is-note-link" href="3_hw1MPI_1_16_20.html">3. Homework 1 Review / Intro. to MPI</a></li><li><a class="is-note-link" href="4_communicationLatency_1_21_20.html">4. Communication, Deadlock, and Latency</a></li><li><a class="is-note-link" href="5_permuationsCollectiveComm_1_23_20.html">5. Permutations and Collective Communication</a></li><li><a class="is-note-link" href="6_collectivesPrefixSum_1_28_20.html">6. Collection Example / Prefix-Sum</a></li><li><a class="is-note-link" href="7_morePrefixSum_1_30_20.html">7. Prefix-Sum Algorithm</a></li><li><a class="is-note-link" href="8_prefixCAllToAll_2_4_20.html">8. Prefix Sum C Example / All-to-All</a></li><li><a class="is-note-link" href="9_bitonicSort_2_6_20.html">9. Bitonic Sort</a></li><li><a class="is-note-link" href="10_parallelQuickSort_2_11_20.html">10. Parallel Quick Sort</a></li><li><a class="is-note-link" href="11_sampleSort_2_20_20.html">11. Sample Sort</a></li><li><a class="is-note-link" href="12_midtermSol_2_25_20.html">12. Midterm Solutions</a></li><li><a class="is-note-link" href="13_introEmbeddings_2_27_20.html">13. Intro. to Embeddings</a></li><li class="active-note-page"><a class="is-note-link" href="14_mpiEmbeddings_3_3_20.html">14. Embeddings in MPI</a></li><li><a class="is-note-link" href="15_parallelMatrices_3_5_20.html">15. Parallel Matrix Operations</a></li><li><a class="is-note-link" href="16_2dMatrixAlgo_3_10_20.html">16. 2D-Matrix Algorithms</a></li><li><a class="is-note-link" href="17_optimal2DMatrix_3_12_20.html">17. Optimal 2D Matrices / Caching</a></li><li><a class="is-note-link" href="18_hw3Review_3_31_20txt.html">18. Homework 3/Exam 2 Review</a></li><li><a class="is-note-link" href="19_introFFT_4_7_20.html">19. Intro. Fast Fourier Transform</a></li><li><a class="is-note-link" href="20_parallelFFT_4_9_20.html">20. Parallel FFT</a></li><li><a class="is-note-link" href="21_parallelGraphAlgos_4_14_20.html">21. Parallel Graph Algorithms</a></li><li><a class="is-note-link" href="22_moreGraphAlgos_4_16_20.html">22. Graph Algorithms (cont.)</a></li><li><a class="is-note-link" href="23_loadBalancing_4_21_20.html">23. Load Balancing (cont.)</a></li></ul>
</nav>
<main>
<a class="side-link is-note-link" href="13_introEmbeddings_2_27_20.html"></a>
<article>
<!-- Actual note text goes into 'pre' -->
<pre id="text-width-ruler"></pre>
<pre class="main-note-text">//****************************************************************************//
//****************** Embeddings in MPI - March 3rd, 2020 ********************//
//**************************************************************************//

- Okay, the (apparent) agenda for today:
    - Embedding example (dense linear algebra)
    - MPI 2: Distributed topologies
    - MPI 3: Neighbor Collectives

- Now, for programming assignment 2
    - MPI_Probe let's you check if a message exists WITHOUT having to actually receive it yet; you can find how big the message is and who it came from, and once you have the message size then you can receive the message when the buffer is ready (I think?)
    - The master thread has to listen to all the messages, but you shouldn't need to worry about too many messages being sent; MPI implementations should be able to handle that behind the scenes for you (by having a large waiting buffer or by delaying sending some messages)
        - Congestion is NOT something you have to worry about for correctness, although it may affect performance
    - Once the master thread has found a partial solution, it should send it out to a worker immediately (it could be technically correct to wait more, but the spirit of the assignment is to have communication and computation overlapping)
--------------------------------------------------------------------------------

- Last time, we started to go through embeddings, and the main idea was that we had some network the algorithm is written for, we have a different physical network, and we need to transform from one to the other (going from the algorithm network to the physical network and then back, e.g. changing from our algorithmic rank to the physical node that process needs to map to)
    - To try and show this, let's try to take a dense matrix-vector product, i.e. we're trying to solve for "y" as follows:

            y = A*x

        - Let's say we want to evenly distribute x/y across all of our resources, and we know we're using 6 MPI processors to run this operation; how can we divide the 2D grid of work among 6 processes?
        - Since 6 isn't a power of 2, let's divide this matrix into a 2x3 grid (i.e. 6 partitions), each part the same size
            - So, we want to transform each section (i,j) into some MPI rank (i.e. the processor that handles it)
            - Then, the vector "x" will be split into 3 partitions (the width of our division), and "y" will be split into 2 (the height of our division)
        - Once we've done that division, each processor will take its chunk, multiply it by the appropriate values from x locally; then, within each row, we'll reduce and scatter (?)
            - Here, we've chosen a row-major embedding where we count across rows first (although we could've done column-major just as well); after we've done the computation on each physical processor, we need to convert BACK to the algorithmic form and receive data from (some other processor f_col_major(i,j) ???), then allgather on our column communicator (???)

- So, how can we use MPI to facilitate stuff like this? In MPI v2, there's support for distributed topologies, and in MPI v3 there's even support for something fun called "neighborhood collectives"
    - So, WAYYYYY back when we first introduced communicators we said that we could think of them as a set of processors that need to work together, each with a unique ID
    - We can create a copy of these communicators, or split them up (MPI_Comm_Split), and when we're done with the communicator we have to free it
    - Besides just giving an ID to each processor, though, we can ALSO say who should be talking with whom, assigning a VIRTUAL TOPOLOGY to the MPI communicator
        - These virtual topologies don't have to be the same as the physical machine, but regardless they can often make implementing an algorithm simpler
        - In general, we can describe these topologies as a multi-edge directed graph, with more heavily connected nodes able to communicate better
            - We can always describe graphs as adjacency matrices; undirected graphs will always have a binary, symmetric matrix
                - A directed graph will still be binary, but now may no longer be symmetric!
                - Finally, a graph with weights will no longer be binary; we can have arbitrary weights!
            - Adjacency matrices are great for nearly-complete graphs, but if most possible connections don't exist then we have a SPARSE matrix, meaning that most of the connections are 0
                - When this happens, we can instead use an ADJACENCY LIST (taking up O(V + E) space, instead O(V^2))
                - What does this look like in memory? There are several possibilities, but a common one is a COMPRESSED ADJACENCY LIST representation, where each vertex has a pointer to a list of vertices reachable from it (basically, 2 lists, one with the vertex indices and another with the edges there) (phony vertex at the end for some reason)
                - Alternatively, we could've just had a list of edges (which will be less compressed)

- How do we specify a virtual topology in MPI? I'm glad you asked!
    - We can use the function MPI_Graph_Create; it'd similar to MPI_Comm_dup, but requires some additional information
        - This one doesn't give us any guarantees about performance, so there's a more in-depth function called MPI_DIST_GRAPH_CREATE_ADJACENT that gives us more control for optimization purposes, letting each process define its own neighborhood
    - So, let's say we wanted to create a binary tree topology like this:

                    0
                  /   \
                1       2
               / \     / \
              3   4   5   6

        - How can we create this? One way to do it in MPI:

                // TODO: Not sure if this is for a binary tree or a hypercube?
                // Basic setup info
                int nnodes = 0;
                MPI_Comm_Size(comm, &amp;nnodes);

                int k = static_cast&lt;int&gt;(log2(nnodes));
                int *index = new int[nnodes];

                (...rest of the code on the slides...)

    - One particularly common topology in MPI and parallel computing is CARTESIAN TOPOLOGY, which we can make using MPI_Crt_Create (defining the number of dimensions, if we want it to be a ring or just a grid, )
        - We can then get the coordinates of the processor in MPI, both as a physical rank and algorithmically:

                MPI_Cart_Rank(comm, coords, rank)
                MPI_Cart_Coords(...)

        - Shifting is a common operation in these types of grids, which we can do using MPI_Cart_Shift (NOTE that this doesn't actually send a message; see API for details)

- Rather than trying to work through a complicated example of this, let's talk about a fun thing that can simplify a LOT of collective operations: neighborhood collectives!
    - Let's say we have an operation that needs to know the results of the 4 neighboring processors around it (like for a finite difference fluid simulation or something)

                            (i, j-1)
                               |
                (i-1, j)-----(i, j)-----(i+1, j)
                               |
                            (i, j+1)

    - Basically, we want to do a very small "all to all" with the processor's 4 neighbors, where it gets 4 pieces of information for its neighbors and it sends its own information to those 4 neighbors
        - We could do this by setting up a BUNCH of small communicators, but instead, we can specify a neighborhood in MPI v3 and use "MPI_Neighbor_allgather" to do this instead!

- Alright, that's all for today - Homework 2 supplement is due this Wednesday, and I'll see you on Thursday!
</pre>
</article>
<a class="side-link is-note-link" href="15_parallelMatrices_3_5_20.html"></a>
</main>
</body>
</html>