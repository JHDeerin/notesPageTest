<html>
<head>
<meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0" name="viewport"/>
<title>Jake's CS Notes - Intro. to HPC</title>
<link href="https://fonts.googleapis.com/css?family=Inconsolata" rel="stylesheet"/>
<link href="../../css/testStyle.css" rel="stylesheet"/>
<link href="../../css/notePageStyle.css" rel="stylesheet"/>
<link href="../../css/cx4220Theme.css" id="class-theme-styles" rel="stylesheet"/>
<link crossorigin="anonymous" href="https://use.fontawesome.com/releases/v5.0.13/css/all.css" integrity="sha384-DNOHZ68U8hZfKXOrtjWvjxusGo9WQnrNx2sqG0tfsghAvtVlRW3tvkXWZh58N9jp" rel="stylesheet"/>
</head>
<body>
<script defer="" src="../../js/wrapText.js"></script>
<script defer="" src="../../js/pageTransitions.js"></script>
<script async="" defer="" src="../../js/loadMathJax.js"></script>
<nav class="nav-top">
<ul>
<li class="link-with-slash"><a href="../../index.html"><i class="fas fa-home"></i></a></li>
<li><a href="#" id="class-title-link">Intro. to HPC</a></li>
</ul>
<ul class="note-links-slider"><li><a class="is-note-link" href="0_firstDay_1_7_20.html">0. First Day</a></li><li><a class="is-note-link" href="1_parallelGoals_1_9_20.html">1. Parallel Algorithm Design Goals</a></li><li><a class="is-note-link" href="2_algoExamplesMPI_1_14_20.html">2. Algorithm Examples / Intro. to MPI</a></li><li><a class="is-note-link" href="3_hw1MPI_1_16_20.html">3. Homework 1 Review / Intro. to MPI</a></li><li><a class="is-note-link" href="4_communicationLatency_1_21_20.html">4. Communication, Deadlock, and Latency</a></li><li><a class="is-note-link" href="5_permuationsCollectiveComm_1_23_20.html">5. Permutations and Collective Communication</a></li><li><a class="is-note-link" href="6_collectivesPrefixSum_1_28_20.html">6. Collection Example / Prefix-Sum</a></li><li><a class="is-note-link" href="7_morePrefixSum_1_30_20.html">7. Prefix-Sum Algorithm</a></li><li><a class="is-note-link" href="8_prefixCAllToAll_2_4_20.html">8. Prefix Sum C Example / All-to-All</a></li><li><a class="is-note-link" href="9_bitonicSort_2_6_20.html">9. Bitonic Sort</a></li><li><a class="is-note-link" href="10_parallelQuickSort_2_11_20.html">10. Parallel Quick Sort</a></li><li><a class="is-note-link" href="11_sampleSort_2_20_20.html">11. Sample Sort</a></li><li><a class="is-note-link" href="12_midtermSol_2_25_20.html">12. Midterm Solutions</a></li><li><a class="is-note-link" href="13_introEmbeddings_2_27_20.html">13. Intro. to Embeddings</a></li><li><a class="is-note-link" href="14_mpiEmbeddings_3_3_20.html">14. Embeddings in MPI</a></li><li><a class="is-note-link" href="15_parallelMatrices_3_5_20.html">15. Parallel Matrix Operations</a></li><li><a class="is-note-link" href="16_2dMatrixAlgo_3_10_20.html">16. 2D-Matrix Algorithms</a></li><li class="active-note-page"><a class="is-note-link" href="17_optimal2DMatrix_3_12_20.html">17. Optimal 2D Matrices / Caching</a></li><li><a class="is-note-link" href="18_hw3Review_3_31_20txt.html">18. Homework 3/Exam 2 Review</a></li><li><a class="is-note-link" href="19_introFFT_4_7_20.html">19. Intro. Fast Fourier Transform</a></li><li><a class="is-note-link" href="20_parallelFFT_4_9_20.html">20. Parallel FFT</a></li><li><a class="is-note-link" href="21_parallelGraphAlgos_4_14_20.html">21. Parallel Graph Algorithms</a></li><li><a class="is-note-link" href="22_moreGraphAlgos_4_16_20.html">22. Graph Algorithms (cont.)</a></li><li><a class="is-note-link" href="23_loadBalancing_4_21_20.html">23. Load Balancing (cont.)</a></li></ul>
</nav>
<main>
<a class="side-link is-note-link" href="16_2dMatrixAlgo_3_10_20.html"></a>
<article>
<!-- Actual note text goes into 'pre' -->
<pre id="text-width-ruler"></pre>
<pre class="main-note-text">//****************************************************************************//
//*********** Optimal 2D Matrices / Caching - March 12th, 2020 **************//
//**************************************************************************//

- Today's glorious agenda:
    - Homework 3 (posted by tonight)
        - due Tuesday after break
    - Study guide posted tomorrow for Exam 2
        - Exam topics:
            - Bitonic sort
            - Sample sort
            - Embeddings
            - Matrix-matrix multiplications
            - MPI for above
        - All of the above will be tested on homework 3 - "I'm planning on one homework problem on each of them"
        - The exam will happen on THURSDAY when you get back
    - In-lecture:
        - Finish matrix-matrix multiplication

- A whole 5 people braved the coronavirus to come to lecture...in a way, I'm impressed, but COME ON!

- Also, example solutions for programming assignment 2 were posted; one is a TAs solution from a past semester that DOESN'T interleave master/worker processing, and the other is Professor Isaac's solution that does try to use that time
    - The basic idea behind this is "what if we split the master thread into 2 parts: a 'backend' and a 'frontend'?" The backend generates new partial solutions, and the frontend gives those solutions to the workers when requested
        - We only have 1 thread, though, so we basically have to implement a poor man's co-routine to handle this on a single thread
--------------------------------------------------------------------------------

- "So, I asked people to read ahead on the lower-bounds of communications needed for matrix-matrix multiplication algorithms"
    - We'll also look at a useful idea we've been dancing around called "iteration space"
    - ...and then talk about how we can use ideas from parallel algorithms to also improve cache performance for serial algorithms

- Let's start by talking about iteration spaces - what are they?
    - So, for matrix multiplication, we're trying to get the result

            C = A x B

    - Where, for individual elements/submatrices:

            C(i,j) = sum from k=0...n { A(ik) x B(kj) }

    - So, a naive serial algorithm for matrix multiplication looks like this:

            for i:
                for j:
                    for k:
                        c[i,j] += a[i,k] * b[k,j]

    - We can represent this space as a 3D cube, with the axes i,j, and k


                |\---------\
                | \    C    \
                |  \         \
                j B k---------
                 \  |         |
                  \ |    A    |
                   \|         |
                     ---------i

        - On "A", we're iterating over the i/k plane, on "B" we're iterating over the j/k plane, and over "C" we're iterating through the i/j plane
            - Inside the cube, a random integer coordinate "xyz" corresponds to some product, a multiplication operation we have to do
            - So, what we've been trying to with our parallel decompositions is to take some portion of these operations and divvy them up among our separate processors, giving each processor a cube of operations or plane of operations or whatnot from this space to eventually use them to compute "C"
        - So, if we take a slice from "A" and "B", and project those slices into the cube, their intersection will form a smaller cube/rectangular prism of operations that're needed, and projecting THAT onto C will give the resulting final values those calculations give us
            - So, assuming we've got those slices, we want to figure out how many operations are needed, and vice-versa: if we have some volume of computations, what inputs will be need from A and B, and what output will we get?

- So, right now, the amount of memory moved by Cannon's algorithm (the best matrix multiplication algorithm we've seen so far) is "n^2/p"
    - ..because to compute an "n*sqrt(p) x n*sqrt(p)" submatrix in C, we need "n" rows, each of size "n/sqrt(p)" size from the input matrices (double-check this?)
    - But is this the BEST we can do? Can we do any better? How many multiplications do we NEED?
        - Well, think of iterations spaces! We have some subset inputs from A "sA" and some subset of input operations "sB" from B, to get some subset of the resulting matrix C "sC"
        - So, in intersection space, the minimum number of computations we need is the intersection of all of these splotches projected inside the cube
            - What's the minimum size this could be? The smallest each input could be is a square "n*sqrt(p) x n*sqrt(p)" on each face, so the smallest intersection we could possibly get is a cube with sides of length "" (???)
            - So, the number of multiplies has to be:

                    &lt;= sqrt(|sA| * |sB| * |sC|)

            - Let's say we split our algorithm into phases, each of which can communicate at most "M" pieces of memory to other processes per phase; then, by the above inequality, each of the subsets has to be &lt;= |2M|
                - That means that, per phase, there can't be any more than:

                        2*sqrt(2) * M^(3/2) multiplications

                - Now, if each round does "W" computations, then the number of rounds "L" must act as follows:

                        L &gt;= W/(2*sqrt(2)*M^(3/2))

                - Therefore, L*M &gt;= w/(2*sqrt(2)*sqrt(M)) - M
            - Thus, if there are "p" processors, at least one of them HAS to perform "m*k*n/p" multiplications, so the entire number of words moved is:

                    m*k*n / (2sqrt(2)*p*sqrt(M)) - M

            - Now, if we're multiplying two NxN matrices, that mean we have a minimum memory requirement of:

                    &gt;= n^3 / (2*sqrt(2)*p*sqrt(M)) - M
                    =&gt; sqrt(3)*n/sqrt(p) ~= sqrt(M)
                    =&gt; 1/sqrt(M) ~= sqrt(3)*sqrt(p)/n

            - So, for Cannon's algorithm, the amount of memory we're trying to send per round is the whole sub-matrix "n^2/p" (I think??), meaning that YES, Cannon's algorithm is optimal!
                - ...not sure I see it?

- So, Cannon's algorithm is optimal in bandwidth, but what about latency?
    - Assuming we only have enough memory to store 1 matrix on each processor, Cannon's is optimal; but if we have more memory, we can use 3D
    - (confused by slides???)

- "So, that's the last algorithm we'll be looking at in the linear algebra part of the class - well, the dense linear algebra part at least"

- That brings us to the last bullet point of the day: how can these parallel algorithm ideas help us write serial algorithms?
    - In the most basic model where we just have memory and a CPU then, by Brent's lemma, it doesn't help us much: serial algorithms will beat parallel ones
        - HOWEVER, in modern processors, almost all computers have a cache of recently-used values - and while a computer architecture class would go super in-depth about where to put things in the cache and so forth, today we'll just assume we have a pretty smart cache that lets the user control where the data goes AND what data gets evicted, so we just need to worry about what memory we access
        - In reality, a good default for real processors is to evict the least recently used piece of memory, although there's a BUNCH of different schemes
    - Let's say we have a cache of size Z, and we write a loop that looks like this:

            for i-1...n
                for j=1...n
                    for k=1...n
                        C[i,j] = A[i,k] * b[k,j]

        - How will the cache behave? Using iteration spaces, we can actually guess: the inner loops are the ones that change the fastest, so we're going upwards and bringing in a strip of A/B each loop, then moving along j to read in new strips of B...
            - If each strip is "N" elements, then let's say the cache can hold N/2 elements - about half of each strip
        - So, currently we're having NO cache reuse despite having ~2*n^3 memory movements to the cache for N^2 of these strips over the whole for loop
    - How can we do better?
        - Theoretically, we can do the most multiplication with values in the cache when we have 1/3 of the cache devoted to A, B, and C, respectively; in this case, the best case we can do is:

                sqrt(Z/3)^3 ~= Z^(3/2)

        - How many times would we have to refill this cache, then? There are n^3 total operations, so if we can do z^3/2 operations per full cache then the total amount of cache refills needed is:

                n^3 / (Z^(3/2)) ~= (n/sqrt(Z))^3

            - Similarly to where we had sqrt(p) in our parallel matrix lower-bound proof (??), we can achieve this optimal cache usage by basically simulating Cannon's algorithm with p = Z^2
                - This essentially does a cube of calculations, then jumps to the next cube; the order doesn't matter too much, but the switch to cubes is HUGE!
            - If we do this, and use the same set of memory for those Z^(3/2) operations, then we've changed the memory usage to:

                    2*n^3/sqrt(Z)

            - So, we've gone from not using the cache at all to getting a speedup proportional to the size of the cache - that's awesome!

- Okay, that's time, so I'll post Homework 3 and the study guide for your perusing pleasure later - have a nice break!
</pre>
</article>
<a class="side-link is-note-link" href="18_hw3Review_3_31_20txt.html"></a>
</main>
</body>
</html>