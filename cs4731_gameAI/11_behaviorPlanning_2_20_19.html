<html>
<head>
<meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0" name="viewport"/>
<title>Jake's CS Notes - Game AI</title>
<link href="https://fonts.googleapis.com/css?family=Inconsolata" rel="stylesheet"/>
<link href="../css/testStyle.css" rel="stylesheet"/>
<link href="../css/notePageStyle.css" rel="stylesheet"/>
<link href="../css/cs4731Theme.css" id="class-theme-styles" rel="stylesheet"/>
<link crossorigin="anonymous" href="https://use.fontawesome.com/releases/v5.0.13/css/all.css" integrity="sha384-DNOHZ68U8hZfKXOrtjWvjxusGo9WQnrNx2sqG0tfsghAvtVlRW3tvkXWZh58N9jp" rel="stylesheet"/>
</head>
<body>
<script defer="" src="../js/pageTransitions.js"></script>
<nav class="nav-top">
<ul>
<li class="link-with-slash"><a href="../index.html"><i class="fas fa-home"></i></a></li>
<li><a href="#" id="class-title-link">Game AI</a></li>
</ul>
<ul class="note-links-slider"><li><a class="is-note-link" href="0_firstDay_1_7_19.html">0. First Day</a></li><li><a class="is-note-link" href="1_gameVsRegularAI_1_9_19.html">1. Game AI vs Regular AI</a></li><li><a class="is-note-link" href="2_introPathfinding_1_14_19.html">2. Intro to Pathfinding</a></li><li><a class="is-note-link" href="3_pathNetworks_1_16_19.html">3. Path Networks</a></li><li><a class="is-note-link" href="4_morePathNetworks_1_23_19.html">4. Path Networks (cont.)</a></li><li><a class="is-note-link" href="5_pathfindingBasics_1_30_19.html">5. Pathfinding Basics</a></li><li><a class="is-note-link" href="6_pathfindingCont_2_4_19.html">6. Pathfinding (cont.)</a></li><li><a class="is-note-link" href="7_groupsDecisions_2_6_19.html">7. Groups and Decision Making</a></li><li><a class="is-note-link" href="8_moreDecisionMaking_2_11_19.html">8. Decision Making (cont.)</a></li><li><a class="is-note-link" href="9_behaviorTrees_2_13_19.html">9. Behavior Trees</a></li><li><a class="is-note-link" href="10_rulesPlanning_2_18_19.html">10. Rules and Planning</a></li><li class="active-note-page"><a class="is-note-link" href="11_behaviorPlanning_2_20_19.html">11. Behavior Planning (cont.)</a></li><li><a class="is-note-link" href="12_hierarchicalTaskNetworks_2_27_19.html">12. Hierarchical Task Networks</a></li><li><a class="is-note-link" href="13_introProceduralGen_3_4_19.html">13. Intro. to Procedural Generation</a></li><li><a class="is-note-link" href="14_moreProcGen_3_6_19.html">14. Procedural Generation / Terrain Generation</a></li><li><a class="is-note-link" href="15_optimizationAlgos_3_11_19.html">15. Optimization Algorithms</a></li><li><a class="is-note-link" href="16_geneticAlgos_3_13_19.html">16. Genetic Algorithms (cont.)</a></li><li><a class="is-note-link" href="17_playerModeling_3_26_19.html">17. Player Modeling</a></li><li><a class="is-note-link" href="18_introRL_3_27_19.html">18. Intro. to Reinforcement Learning</a></li></ul>
</nav>
<main>
<a class="side-link is-note-link" href="10_rulesPlanning_2_18_19.html"></a>
<article>
<!-- Actual note text goes into 'pre' -->
<pre class="main-note-text">//****************************************************************************//
//************ Behavior Planning (cont.) - February 20th, 2019 **************//
//**************************************************************************//

- Ah, the rain - cool and refreshing, or in this case freezing and 
pneumonia-causing
- Also, because of the delayed midterm (which is on MONDAY, by the way), we ARE 
going to delay homework 5 by a few days
    - Homework 5 isn't "hard," per se, but it does involve figuring out the 
    function calls
- The midterm can cover anything we've talked about so far, up to and INCLUDING 
what we're going to talk about today 
--------------------------------------------------

- Last time we started getting into behavior planning, which is really the 
first decision-making strategy we've talked about that feels more like 
"artificial intelligence" (planning, searching, etc.) than a clever trick
    - We started comparing and contrasting pathfinding and behavior-planning 
    with A*, because even though they use the same algorithm there are som 
    pretty siginificant differences:
        - With pathfinding, a lot of stuff is pretty intuitive: a grid is a 
        decent representation of a physical space, the actions are pretty 
        limited and simple (one movement action isn't too preferable to 
        another, i.e. we don't 'want' to move left inherently any more than 
        moving right), the distance heuristic makes sense, etc.
        - Behavior planning, on the other hand, is more complicated; the 
        actions are very different, comparing them is more apples-to-oranges, 
        and it's difficult to estimate how "far away" we are from reaching a 
        goal

- As an example, here's a planning system about simulating airplanes that uses 
STRIPS-style actions ("Stanford Research in Planning Systems")
    - We have a state like this, listing the airports and paths available to us:

            State: airport(LAX), airport(ATL), at(plane1, ATL), at(plane2, 
            LAX), path(ATL, LAX), !at(plane1, LAX), etc.

    - With the following action (in this case, the action takes in parameters, 
    although it doesn't NEED to):

            Fly(?plane, ?from, ?to):
                Precondition (checking we're at the right spot, that plane 
                really is a plane, path between airports exists, etc.): 
                at(?plane, ?from), plane(?plane), airport(?from), airport(?to), 
                path(?from, ?to), ?from != ?to
                    - Side note: Even though this last rule in the model 
                    prevents the airport we're at from being the destination, 
                    this has happened to Professor Riedl several times in 
                    Atlanta ("It isn't the greatest feeling when your plane 
                    turns around")
                Effect (update the state variables we're changing): at(?plane, 
                ?to), !at(?plane, ?from)

        - So, if all the variables check out and the path exists, we're good!

- A more complicated example from the slides: a bank robbery!
    - Here, the state keeps track of if the man is a criminal yet, if he has a 
    gun, if he has ammo, if he's hungry, if he has money, etc.
        - There might be actions like "RobBank," "LoadGun" (if we have the gun 
        and ammo), "ShootGun" (if gun is loaded), "BuyGun" (if you have money 
        and you're not a criminal - "background checks, right?"), "HuntPossum", 
        "EatPossum," "TakeBath," etc.
            - "...it's not a very realistic bank robbery simulation, okay?"
    - So, we can't lay these states out in a physical map like we could for 
    pathfinding, but we CAN still lay out a state space with the possible 
    actions we can take and the successor states they'll get us to
        - For instance, if we "haveMoney" and we're "!isCriminal," then we can 
        take the "BuyGun" action to reach the state of losing our money and 
        having a gun
            - Similarly, if we have money, we can "takeBath" to lose our money!
            - On the other hand, if we don't "haveGun," then we can't take the 
            "loadGun" option
        - So, with this, we've got ourselves a successor function that tells us 
        which states we can reach given our current conditions - that's an 
        ingredient for A*!
    - So, let's say our goal is to reach the state of being rich and not being 
    hungry - knowing that, and our successor function, we can search through 
    our state space to find a path to a state that fulfills that goal!
- ...but what can we do to get a heuristic? Without knowing how close we are to 
the goal, this is just breadth-first search!
    - So, what're some ideas?
        - We could use the number of actions to get to the goal, but we don't 
        know how many actions it'll take to reach the goal until we've solved 
        the problem
        - We could do the number of goal conditions met, but we might meet a 
        few conditions and still not be able to reach the goal - besides, it's 
        a monotonically increasing condition, so it'll get us there anyway (?)
    - What CAN we do instead, then?
        - With pathfinding, straight-line distance worked so well because it 
        was a "relaxation" of the problem; we pretended we didn't have to worry 
        about obstacles, simplifying the problem and getting us an 
        underestimating cost-estimate
            - This is the whole idea of heuristics, right? Simpler versions of 
            the problem so that we don't have to deal with NP-hard type stuff!

- So, how can we relax our problem here in the STRIPS model? Here's what those 
dorks at Stanford came up with:
    - 1) Only consider POSITIVE effects
        - So, positive effects are traits that are "true" in the problem, like 
        "gunLoaded" or "isRich" as opposed to "~gunLoaded" or "~possumAlive"
            - A BIG complication in these sorts of planning problems are when 
            things go from true to false, which makes order matter (e.g. if we 
            shootPossum before we robBank, then our gun isn't loaded and we 
            can't rob it - but if we robBank first, then we have money, so we 
            can buy more ammo and then shootPossum)
        - If we pretended that "shootPossum" didn't unload our gun, though, or 
        that we can always buy our gun, then actions never get eliminated 
        halfway through our plan, which makes the problem easier!
            - In fact, this means all plans can be generated in linear time!
    - 2) Consider each condition in the current state as independent of the 
    others

- So, to actually calculate the heuristic, we'll do something like this:

        heuristic(state):
            for each condition in current state:
                get minimum # of actions between that condition and the goal
            take the maximum of all conditions in current state as the 
            heuristic value (so that we get the closest underestimate)

    - There's some magic going on here: how do we know how many actions are 
    between each condition and the goal? (got a little confused here)
        - So, let's say our current state has the positive effects "hasGun," 
        "possumAlive," and "has"
        - Knowing the goal node, we'll construct a mini-search problem where we 
        ignore all the negative effects in our state/action (just pretend they 
        don't exist; for now, we'll assume none of our goal conditions are 
        negative); we'll start at the goal and then work BACKWARDS
            - So, all the adjacent nodes to the goal must've transformed some 
            condition we have to a positive one, which means they took in some 
            conditions and gave the output goal condition
                - We say the goal conditions are "0-steps" from the goal, while 
                the input conditions that allowed us to take those actions are 
                "1-step" from the goal
                - We'll then look backwards at the actions that got us to THAT 
                node, and any input conditions that allowed us to take the 
                action that got us to that 1-step-away condition will be 
                2-steps away
                    - We save all these values in a table; if the same 
                    condition ever has 2 different steps-away values, then we 
                    save the max value
        - So, we do this search on a simplified version of the state space map 
        once we know the goal, it takes polynomial time (run until our 
        'openList' from the goal runs out), and then we save all the 
        "steps-away" values in a table
            - So, we calculate this whole search to get the table ONCE when we 
            get our goal node; then, for each "successor()" call, we just look 
            up the steps-away values in the table and say our heuristic is the 
            max applicable one
    - This magical technique has *drumroll* the WORST name ever: "heuristic 
    search planning"
        - "...I don't care what you kids say, I think it's cool"

- OKAY - phew.

- There's ONE more thing to talk about when it comes to decision making - 
hierarchical planning - but I think we'll save that for after the exam. So, 
'til then, happy studying!

</pre>
</article>
<a class="side-link is-note-link" href="12_hierarchicalTaskNetworks_2_27_19.html"></a>
</main>
</body>
</html>