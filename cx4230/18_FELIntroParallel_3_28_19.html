<html>
<head>
<meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0" name="viewport"/>
<title>Jake's CS Notes - Intro. Modeling/Simulation</title>
<link href="https://fonts.googleapis.com/css?family=Inconsolata" rel="stylesheet"/>
<link href="../css/testStyle.css" rel="stylesheet"/>
<link href="../css/notePageStyle.css" rel="stylesheet"/>
<link href="../css/cx4230Theme.css" id="class-theme-styles" rel="stylesheet"/>
<link crossorigin="anonymous" href="https://use.fontawesome.com/releases/v5.0.13/css/all.css" integrity="sha384-DNOHZ68U8hZfKXOrtjWvjxusGo9WQnrNx2sqG0tfsghAvtVlRW3tvkXWZh58N9jp" rel="stylesheet"/>
</head>
<body>
<script defer="" src="../js/pageTransitions.js"></script>
<nav class="nav-top">
<ul>
<li class="link-with-slash"><a href="../index.html"><i class="fas fa-home"></i></a></li>
<li><a href="#" id="class-title-link">Intro. Modeling/Simulation</a></li>
</ul>
<ul class="note-links-slider"><li><a class="is-note-link" href="0_introConceptualModels_1_8_19.html">0. Introduction / Conceptual Models</a></li><li><a class="is-note-link" href="1_complexSystemBasics_1_10_19.html">1. Complex System Basics</a></li><li><a class="is-note-link" href="2_mathReview_1_15_19.html">2. Math Review for Modeling</a></li><li><a class="is-note-link" href="3_stabilityBifurcations_1_17_19.html">3. Stability and Bifurcations</a></li><li><a class="is-note-link" href="4_1dBifurcations_1_22_19.html">4. 1D Bifurcations (cont.)</a></li><li><a class="is-note-link" href="5_2dDiffEqs_1_24_19.html">5. 2D Differential Equations</a></li><li><a class="is-note-link" href="6_2dLinearStability_1_31_19.html">6. 2D Linear System Stability</a></li><li><a class="is-note-link" href="7_2dBifurcations_2_5_19.html">7. 2D Bifurcations</a></li><li><a class="is-note-link" href="8_hopfBifurcations_2_12_19.html">8. Hopf Bifurcations</a></li><li><a class="is-note-link" href="9_chaosIteratedMaps_2_14_19.html">9. Chaos and Iterated Maps</a></li><li><a class="is-note-link" href="10_moreIteratedMapsCellAuto_2_19_19.html">10. Iterated Maps (cont.) / Cellular Automata</a></li><li><a class="is-note-link" href="11_cellularAutomata_2_21_19.html">11. Cellular Automata (cont.)</a></li><li><a class="is-note-link" href="12_discreteEventSim_2_28_19.html">12. Discrete Event Simulation</a></li><li><a class="is-note-link" href="13_queueProcessScheduling_3_5_19.html">13. Queuing and Process Scheduling</a></li><li><a class="is-note-link" href="14_processSchedulingImpl_3_7_19.html">14. Process</a></li><li><a class="is-note-link" href="15_activityScanningRNG_3_12_19.html">15. Activity Scanning / Random Number Generation</a></li><li><a class="is-note-link" href="16_inputAnalysis_3_14_19.html">16. Input Analysis</a></li><li><a class="is-note-link" href="17_outputAnalysisValidation_3_26_19.html">17. Output Analysis / Validation</a></li><li class="active-note-page"><a class="is-note-link" href="18_FELIntroParallel_3_28_19.html">18. Future Event List / Intro. to Parallel</a></li></ul>
</nav>
<main>
<a class="side-link is-note-link" href="17_outputAnalysisValidation_3_26_19.html"></a>
<article>
<!-- Actual note text goes into 'pre' -->
<pre class="main-note-text">//****************************************************************************//
//******* Future Event List / Intro. to Parallel - March 28th, 2019 *********//
//**************************************************************************//

- Alright, you should be working hard on your checkpoints, so one again ,we're 
expecting:
    - Your problem statement / conceptual model
    - (we might NOT need the empirical distribution done yet?)
        - "You should've started on it, and you'll need it for your final 
        project, but it's okay if your simulations for the checkpoint are using 
        placeholder data"
    - A "working version" of your simulator - it should be as BASIC as you can 
    make it right now for the checkpoint, and then after that's past 
    - You can submit 

------------------------------------------------------

- Alright, we've covered most of the "modeling lifecycle," and we're starting 
to get into implementation issues and parallel processing - but let's start off 
by revisiting the future event list

- As you know, the future event list is just a priority queue, but there's been 
a LOT of research into different ways to actually implement this
    - Again, a future event list holds all the events we're waiting to process, 
    and because it holds almost every event we care about it has a HUGE impact 
    on performance
        - If the performance of this is good, we're 
        - Per-event computation time usually isn't very high, but having the 
        check and search through this list 
    - So, what operations do we NEED for this list?
        - Some way of inserting events with a given timestamp, some way of 
        deleting the smallest timestamp event, and a way of deleting arbitrary 
        events (to handle interrupts, or unexpected situations where scheduled 
        events are now invalid)
        - If we're making a general-purpose event list, we also need a few 
        constraints:
            - 
    - These requirements SCREAM linked list as an obvious data structure 
    choice, right? But is it obviously the best?
        - Tree-based structures, like heaps and splay trees, are also pretty 
        common - having a worst-case access time of log(n) is MUCH better
        - In particular, some priority queues based on hashing can give us 
        constant-time performance, which is great!

- To evaluate the performance of a particular FEL, we'll usually use the HOLD 
model to do empirical performance evaluation
    - This is based on the "hold" primitive (or "AdvanceTime" - they're the 
    same thing), where for a given fixed queue size we'll schedule a new event 
    and delete an existing one (enqueue + dequeue time)
        - Using this model, we can see that linear linked lists are the fastest 
        for small lists (&lt; 100 elements), but it EXPLODES if you have a large 
        event list
        - At large sizes, then, we need more efficient data structures, like 
        tree-based heaps 
    - Now, results will vary a bit between machine architectures

- Hullo, did I say "constant time" somewhere? That's exciting! I WANT, HOW GET?
    - Well, we do this by using hashing-style methods - and for priority 
    queues, that means CALENDAR QUEUES
        - The idea behind calendar queues is that we'll have an array of 
        buckets, each covering some discrete interval of a time (e.g. 365 
        buckets for the days of a year)
            - Each "bucket" will have a simple singly-linked list queue to hold 
            the events that fall inside that bucket (sorted or otherwise)
        - We'll then have a pointer to the current bucket/day, so that we can 
        get to it right away
    - And that's it!
        - Insertion is pretty easy: take the timestamp, map it to the 
        approporiate bucket, and throw it into that bucket's queue
        - Deleting, though, is a little more tricky:
            - We'll start by looking in today's bucket
    - A few problems come to mind with this, though:
        - Within the buckets, we still might get a long list
        - When deleting, we also might need to search through a lot of buckets
    - To avoid this, if the number of events in the queue gets too large we'll 
    occasionally resize the calendar so that there are only 1-2 events 
        - To do this, we literally just make a larger event calendar, and then 
        copy all of our existing events over to the new one - it's expensive
- How's the performance of this? Well, for a small number of events, linear 
lists still win - but for large queues, the HOLD model shows it massively 
outperforming other
    - ...but remember something important about the HOLD model? The list size 
    is constant! That means the queue never has to do an expensive resizing 
    operation!
        - As it turns out, the calednar queue performs VERY well when the 
        number of events and the average timestep doesn't change very much, but 
        can approach linear time when these vary - which is AWFUL! It's as bad 
        as a linear linked list!

- Because of those problems, an alternative exists, known as the LADDER QUEUE
    - This is a hybrid data structure, consisting of 3 layers:
        - A TOP layer, which holds an unsorted list of events in the "far 
        future" we're not too worried about
            - Here, we'll just store the max/min timestamps and the number of 
            events
        - A MIDDLE layer, which is a kind of multi-level calendar queue and 
        where the ladder name comes from
            - How does that work? Well, as we mentnioed, the problems with 
            calendar queues all come from a lot of events falling into a single 
            bucket
            - To fix this, if too many events fall into a single bucket, we'll 
            spawn a new calendar queue JUST to hold that bucket's events!
                - And if a bucket in THAT queue fills up too much, then 
        - A BOTTOM layer, which is a sorted list holding the "near future" 
        events we're most worried about
            - This is the layer we'll dequeue from most often
    - So, the big ideas behind this data structure are that:
        - Each "rung" of our middle-layer level has a different bucket size
        - we try to delay sorting until events are just about to be dequeued; 
        insertions happen to the middle/top layer and aren't sorted at all, 
        which means our resizing calendars don't 

- So, in summary, priority queue performance is an important issue, ESPECIALLY 
for very large simulations with many millions of events
    - Linear list is the fastest if things are small, O(log n) structures like 
    heaps and splay trees are reliable, Calendar queue is blazingly fast but 
    with some major downsides in some cases, and Ladder queues are complicated 
    to implement but can often give us the best of both worlds

- Now, for the rest of the semester, we'll be talking about parallel event 
simulations - so let's dive right in
    - Now, PARALLEL SIMULATIONS basically means we're taking a single program, 
    chopping it up into pieces, and executing on multiple TIGHTLY COUPLED 
    processors (e.g. cores all sharing memory on a board)
        - DISTRIBUTED SIMULATIONS still involve executing a single program, but 
        we're now running them on LOOSELY coupled processors, e.g.
        - There's a 3rd type, REPLICATED TRIALS, that we don't care as much 
        about as much
- Years ago, we cared about parallel processing because researchers had access 
to supercomputers and wanted to take advantage of that extra power to do things 
faster, or because their program needed more memory than a single computer had
    - Nowadays, though, parallel processing is EVERYWHERE: every laptop 
    processor has multiple cores!
    - We might also care because we have resources that are distributed over a 
    large space (?)
    - Interoperability (?)

- So, that's the definitions, but how did we get here in history?
    - Briefly, parallel processing started in the "High-Performance Computing 
    Community" in the early 1980s with the "Chandy-Misra-Bryant" and "Time 
    Warp" algorithms, which are respectively known as "conservative" and 
    "optimistic" synchronization algorithms
    - At the same time, the defense community was having trouble with many 
    different systems that didn't work together, and in 1983 they started the 
    SIMNET project to emphasize reusing old simulations for new purposes
        - They started pushing for standards to keep their simulations 
        interconnected, and this continued into the 1990s
        - The military was also interested in having multiple computer 
        simulations that they could hook together
    - These defense initiatives eventually led to the IEEE standard "High Level 
    Architecture," which was first standardized in 1996 and was last revised in 
    2010
- Finally, yet another push for parallel simulation came from the multiplayer 
gaming community in the 1990s, which needed to figure out how to get

- What does a parallel discrete event simulation look like, then?
    - Well, for each core/computer we're running the simulation on, we'll have 
    a LOGICAL PROCESS (LP) that can schedule events for other processess 
    (which'll require sending messages to other logical processes)
    -
- For example, think about our airport example
    - We'll have a process for each airport
    - All information we share between
- What do the code changes look like for this? Not much!
    - The ONLY change to the airport code is that when we're processing a 
    departure, we'll schedule an arrival event at a DIFFERENT airport (which'll 
    involve), and the time to get there


- So, that sounds really easy! But what's the catch?
    - At first, it seems like this LP thing is suited to concurrent events 
    really naturally - we're just changing a physical process to a logical 
    process and each physical interaction into an event/message
    - The problem is that, when we had everything on a single processor, we had 
    a LOCAL CAUSALITY CONSTRAINT: our single future event list guaranteed we 
    processed all events in time-stamp order
        - The problem is that when we begin to process an event, there's a 
        chance that another process will add a new event that should happen 
        BEFORE that one (e.g. what if we're about to ) 
    - This is known as the SYNCHRONIZATION problem, and it's a big one: how can 
    we ensure that all events are processed in the right order across ALL 
    processes?
- If we can ensure that all these events are synchronized, we're good - we're 
guaranteed

- So, we need an algorithm to ensure these things stay in sync, and there are 2 
broad categories of algorithms to do that:
    - CONSERVATIVE algorithms try to always avoid violating the LCC until we 
    know it's safe; they try to avoid making any mistakes
        - The problem here is avoiding deadlock
    - OPTIMISTIC ALGORTIHMS allow these violations to occur, but then try to 
    fix them; it allows errors to happen but tries to correct them as it runs

- One of the oldest, most famous conservative algorithms is the 
CHANDY-MISRA-BRYANT ALGORITHM (or "Null Message algorithm")
    - To use this algorithm, we're going to make a few assumptions:
        - All the logical processes are exchanging messages with time stamps
        - The network isn't changing (i.e. no logical processes are being 
        created or deleted)
        - Messages on each link are sent in time-step order (i.e. from a given 
        process, any later-sent messages by that process will really be later)
        - The network delivers things reliably in their sent order
    - Based on those above assumptions, we know that the last message received 
    on a specific link will be GUARANTEED to be the the lowest timestamp from 
    that process
        - How does that help us?

    - so, the simplest way we can use this is an algorithm like this:

            while running:
                Wait until each FIFO contains at least one message
                Remove smallest time stamped event from all FIFOs


        - This looks good, right? But the problem is that this can lead to 
        DEADLOCK: two separate LPs can be waiting to receive a message from the 
        other, which stops the whole system!
    - To get around this, the CMB algorithm will have each LP send a NULL 
    MESSAGE saying the minimum time it'll take for it to send a new message
        - So, let's say that we're at timestep 0, and a process gives us a null 
        message saying that

- We'll 

</pre>
</article>
<a class="side-link is-note-link" href="18_FELIntroParallel_3_28_19.html"></a>
</main>
</body>
</html>